{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, time\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from wzh.transformer import Transformer\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\"\n",
    "\n",
    "n_class = 10\n",
    "\n",
    "patch_shape = (4, 4)\n",
    "d_patch = math.prod(patch_shape) * 3\n",
    "n_patch = 3 * 32 * 32 // d_patch\n",
    "\n",
    "dropout = 0.0\n",
    "epochs = 20\n",
    "batch_size = 384\n",
    "learn_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim_model = 384\n",
    "        self.embedding = nn.Linear(d_patch, dim_model)\n",
    "        self.model = Transformer(\n",
    "            nlayer=6,\n",
    "            dim_model=dim_model,\n",
    "            num_head=8,\n",
    "            max_seq_len=n_patch,\n",
    "            glu_attn=False,\n",
    "        )\n",
    "        self.output = nn.Linear(dim_model, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.unfold(x, patch_shape, stride=patch_shape).mT\n",
    "        x = self.embedding(x)\n",
    "        x = self.model(x)\n",
    "        x = self.output(x)\n",
    "        x = x.mean(-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLUAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim_model = 384\n",
    "        self.embedding = nn.Linear(d_patch, dim_model)\n",
    "        self.model = Transformer(\n",
    "            nlayer=6,\n",
    "            dim_model=dim_model,\n",
    "            num_head=8,\n",
    "            max_seq_len=n_patch,\n",
    "            glu_attn=True,\n",
    "        )\n",
    "        self.output = nn.Linear(dim_model, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.unfold(x, patch_shape, stride=patch_shape).mT\n",
    "        x = self.embedding(x)\n",
    "        x = self.model(x)\n",
    "        x = self.output(x)\n",
    "        x = x.mean(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    total_time = time.time()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    avg_loss, acc = 0, 0\n",
    "    for i, (source, target) in enumerate(dataloader):\n",
    "        source: Tensor = source.to(device, non_blocking=True)\n",
    "        target: Tensor = target.to(device, non_blocking=True)\n",
    "        pred = model(source)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            avg_loss += loss.item()\n",
    "            acc += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    avg_loss /= num_batches\n",
    "    acc /= size\n",
    "    total_time = time.time() - total_time\n",
    "    return (acc, avg_loss, total_time)\n",
    "\n",
    "\n",
    "def val(dataloader, model, loss_fn):\n",
    "    total_time = time.time()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    avg_loss, acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for source, target in dataloader:\n",
    "            source: Tensor = source.to(device, non_blocking=True)\n",
    "            target: Tensor = target.to(device, non_blocking=True)\n",
    "            pred = model(source)\n",
    "            avg_loss += loss_fn(pred, target).item()\n",
    "            acc += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    avg_loss /= num_batches\n",
    "    acc /= size\n",
    "    total_time = time.time() - total_time\n",
    "    return (acc, avg_loss, total_time)\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandAugment(num_ops=2, magnitude=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.RandomErasing(\n",
    "            p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train,\n",
    ")\n",
    "val_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list: list[nn.Module] = [Baseline, GLUAttention]\n",
    "\n",
    "for model_creator in model_list:\n",
    "    for i in range(10):\n",
    "        model = model_creator().to(device)\n",
    "        if i == 0:\n",
    "            print(model)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_data, batch_size, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), learn_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "        train_total_time = 0\n",
    "        val_total_time = 0\n",
    "        print(\n",
    "            \"epoch, train acc,   val acc,train loss,  val loss,train time,  val time,total time\"\n",
    "        )\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            (train_acc, train_loss, train_time) = train(\n",
    "                train_dataloader, model, loss_fn, optimizer\n",
    "            )\n",
    "            scheduler.step()\n",
    "            (val_acc, val_loss, val_time) = val(val_dataloader, model, loss_fn)\n",
    "            train_total_time += train_time\n",
    "            val_total_time += val_time\n",
    "\n",
    "            print(\n",
    "                f\"{epoch:>5},{train_acc:>10.3f},{val_acc:>10.3f},{train_loss:>10f},{val_loss:>10f},{train_time:>10.1f},{val_time:>10.1f},{train_total_time + val_total_time:>10.1f}\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
