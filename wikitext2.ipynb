{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67012821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from wzh.transformer import Transformer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# dataset = load_dataset(\"data/wikitext2\", \"wikitext-2-v1\")\n",
    "dataset = load_dataset(\n",
    "    \"data/wikitext2/\",\n",
    "    data_files={\n",
    "        \"train\": \"train-00000-of-00001.parquet\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-tokenizer\")\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim_model = 384\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.model = Transformer(\n",
    "            nlayer=6,\n",
    "            dim_model=dim_model,\n",
    "            num_head=8,\n",
    "            max_seq_len=1024,\n",
    "            glu_attn=False,\n",
    "        )\n",
    "        self.output = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.model(x, mask)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLUAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim_model = 384\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.model = Transformer(\n",
    "            nlayer=6,\n",
    "            dim_model=dim_model,\n",
    "            num_head=8,\n",
    "            max_seq_len=1024,\n",
    "            glu_attn=True,\n",
    "        )\n",
    "        self.output = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.model(x, mask)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def prepare_data(example):\n",
    "    tokens = tokenizer(example[\"text\"], truncation=True, max_length=1024)\n",
    "    return {\"input_ids\": tokens[\"input_ids\"], \"labels\": tokens[\"input_ids\"]}\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_data, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in examples]\n",
    "    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    labels = input_ids.clone()\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "def train(model, num_epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    print(f\"parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(model)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_token_list = []\n",
    "    loss_list = []\n",
    "    ema_loss = 8\n",
    "    total_tokens = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            seq_len = input_ids.size(1)\n",
    "            if seq_len == 0:\n",
    "                continue\n",
    "            total_tokens += seq_len\n",
    "            mask = torch.triu(\n",
    "                torch.ones((seq_len, seq_len), dtype=torch.bool, device=device),\n",
    "                diagonal=1,\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, mask)\n",
    "            loss = criterion(\n",
    "                logits[:, :-1].view(-1, vocab_size), labels[:, 1:].view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema_loss = 0.999 * ema_loss + 0.001 * loss.item()\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"ema loss\": f\"{ema_loss:.4f}\",\n",
    "                }\n",
    "            )\n",
    "            num_token_list.append(total_tokens)\n",
    "            loss_list.append(loss.item())\n",
    "        scheduler.step()\n",
    "    return num_token_list, loss_list\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_and_average(list, num_splits=100):\n",
    "    split_indices = np.linspace(0, len(list), num_splits + 1, dtype=int)\n",
    "    avg = []\n",
    "\n",
    "    for i in range(len(split_indices) - 1):\n",
    "        start_idx = split_indices[i]\n",
    "        end_idx = split_indices[i + 1]\n",
    "        avg.append(np.mean(list[start_idx:end_idx]))\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list, loss_list = train(Baseline(), 10)\n",
    "token_list = split_and_average(token_list, 100)\n",
    "loss_list = split_and_average(loss_list, 100)\n",
    "print(token_list)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914af448",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list, loss_list = train(GLUAttention(), 10)\n",
    "token_list = split_and_average(token_list, 100)\n",
    "loss_list = split_and_average(loss_list, 100)\n",
    "print(token_list)\n",
    "print(loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
