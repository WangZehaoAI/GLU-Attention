GLU Attentionåˆ›æ–°çš„æŠŠGLUæœºåˆ¶å¼•å…¥Attentionä¹‹ä¸­ï¼Œå¢å¼ºäº†Transformerçš„æ¨¡å‹æ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ï¼Œæ— é¢å¤–å‚æ•°å¼€é”€ï¼Œå¾®ä¹å…¶å¾®çš„é¢å¤–è®¡ç®—å¼€é”€ã€‚å¯¹åŸå§‹Attentionçš„æ”¹åŠ¨æå°ï¼Œå¹¶ä¸”å¯æ— ç¼é€‚é…å¤šç§Attentionå˜ä½“ï¼Œå¤§å®¶å¯ä»¥å¿«é€Ÿåº”ç”¨åˆ°è‡ªå·±çš„Transformeré¡¹ç›®ä¹‹ä¸­ï¼Œå…è´¹æå‡æ€§èƒ½ã€‚å¸Œæœ›å¤§å®¶éƒ½èƒ½ç”¨èµ·æ¥ğŸ˜ƒ

GLU Attention provide nearly cost-free performance boost for transformers with a simple mechanism that applies Gated Linear Unit to the values in Attention.

# Multi-Head Attention:
$$q=W_q(q)$$
$$k=W_k(k)$$
$$v=W_v(v)$$
$$o=W_o(MHA(q,k,v))$$

# GLU Multi-Head Attention
$$q=W_q(q)$$
$$k=W_k(k)$$
$$v=W_v(v)$$
$$v1,v2=split(v,dim=-1)$$
$$v=v1*silu(v2)$$
$$o=W_o(MHA(q,k,v))$$

By this simple modification both training efficiency and model performance is boosted.


![Cifar-10 training loss of each epoch. The lower the better.](./paper/cifar10_train_loss.png)
Cifar-10 training loss of each epoch. The lower the better.


![Cifar-10 validation accuracy of each epoch. The higher the better.](./paper/cifar10_val_acc.png)
Cifar-10 validation accuracy of each epoch. The higher the better.


![wikitext2 training loss. The lower the better.](./paper/wikitext2_train_loss.png)
wikitext2 training loss. The lower the better.


![wikitext103 training loss. The lower the better.](./paper/wikitext103_train_loss.png)
wikitext103 training loss. The lower the better.

